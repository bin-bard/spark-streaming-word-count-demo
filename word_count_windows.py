from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# Kh·ªüi t·∫°o Spark Context
sc = SparkContext("local[2]", "NetworkWordCount")

# T·∫°o StreamingContext, x·ª≠ l√Ω d·ªØ li·ªáu theo t·ª´ng batch 2 gi√¢y.
ssc = StreamingContext(sc, 2)

# T·∫°o DStream, nh·∫≠n d·ªØ li·ªáu vƒÉn b·∫£n t·ª´ socket localhost:9999 (netcat).
lines = ssc.socketTextStream("localhost", 9999)

# flatMap: T√°ch m·ªói d√≤ng nh·∫≠n ƒë∆∞·ª£c th√†nh c√°c t·ª´ ri√™ng l·∫ª.
# V√≠ d·ª•: "hello world" -> "hello", "world"
words = lines.flatMap(lambda line: line.split(" "))

# üîé L·ªçc: ch·ªâ gi·ªØ l·∫°i c√°c t·ª´ c√≥ ƒë·ªô d√†i > 4
filtered = words.filter(lambda w: len(w) > 4)

# map: Chuy·ªÉn m·ªói t·ª´ th√†nh m·ªôt c·∫∑p (t·ª´, 1) ƒë·ªÉ chu·∫©n b·ªã ƒë·∫øm.
# V√≠ d·ª•: "hello" -> ("hello", 1)
pairs = filtered.map(lambda word: (word, 1))

# reduceByKey: ƒê·∫øm t·∫ßn su·∫•t b·∫±ng c√°ch c·ªông c√°c s·ªë 1 c·ªßa c√°c t·ª´ tr√πng nhau.
# V√≠ d·ª•: ("hello", 1), ("hello", 1) -> ("hello", 2)
wordCounts = pairs.reduceByKey(lambda x, y: x + y)

# In k·∫øt qu·∫£ c·ªßa m·ªói batch ra m√†n h√¨nh.
wordCounts.pprint()

# B·∫Øt ƒë·∫ßu qu√° tr√¨nh x·ª≠ l√Ω streaming.
ssc.start()
# Ch·ªù cho ƒë·∫øn khi ti·∫øn tr√¨nh b·ªã ng·∫Øt.
ssc.awaitTermination()