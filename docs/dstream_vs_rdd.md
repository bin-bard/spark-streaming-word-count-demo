# So sÃ¡nh DStream vs RDD

## ğŸ¯ Tá»•ng quan

**DStream** (Discretized Stream) vÃ  **RDD** (Resilient Distributed Dataset) lÃ  hai khÃ¡i niá»‡m cá»‘t lÃµi trong Apache Spark, nhÆ°ng phá»¥c vá»¥ cÃ¡c má»¥c Ä‘Ã­ch khÃ¡c nhau.

## ğŸ“Š Báº£ng so sÃ¡nh chi tiáº¿t

| Äáº·c Ä‘iá»ƒm | DStream | RDD |
|----------|---------|-----|
| **Äá»‹nh nghÄ©a** | Chuá»—i liÃªn tá»¥c cÃ¡c RDD | Distributed dataset tÄ©nh |
| **Loáº¡i dá»¯ liá»‡u** | Streaming/Real-time | Batch/Historical |
| **Cáº¥u trÃºc bÃªn trong** | Sequence of RDDs over time | Single distributed collection |
| **TÃ­nh báº¥t biáº¿n** | âœ… Immutable | âœ… Immutable |
| **Fault Tolerance** | âœ… Yes (qua lineage) | âœ… Yes (qua lineage) |
| **Lazy Evaluation** | âœ… Yes | âœ… Yes |
| **Caching** | âœ… Yes | âœ… Yes |

## ğŸ”„ Cáº¥u trÃºc dá»¯ liá»‡u

### RDD - Static Dataset
```python
# RDD: Dataset tÄ©nh táº¡i má»™t thá»i Ä‘iá»ƒm
rdd = sc.parallelize([1, 2, 3, 4, 5])
# rdd chá»©a: [1, 2, 3, 4, 5]
```

### DStream - Dynamic Dataset
```python
# DStream: Chuá»—i cÃ¡c RDD theo thá»i gian
lines = ssc.socketTextStream("localhost", 9999)

# T=0s: lines â†’ RDD["hello world"]
# T=2s: lines â†’ RDD["spark streaming"] 
# T=4s: lines â†’ RDD["real time data"]
```

## âš¡ Operations (PhÃ©p toÃ¡n)

### RDD Operations
```python
# Transformations (lazy)
rdd.map(lambda x: x * 2)
rdd.filter(lambda x: x > 3)
rdd.flatMap(lambda x: [x, x*2])

# Actions (eager)  
rdd.collect()
rdd.count()
rdd.take(5)
```

### DStream Operations
```python
# Transformations (lazy)
dstream.map(lambda x: x.upper())
dstream.filter(lambda x: len(x) > 4)
dstream.flatMap(lambda line: line.split())

# Output Operations (eager - trigger execution)
dstream.pprint()
dstream.saveAsTextFiles("output")
dstream.foreachRDD(lambda rdd: rdd.collect())
```

## ğŸ”§ VÃ­ dá»¥ thá»±c táº¿

### Xá»­ lÃ½ vá»›i RDD (Batch)
```python
# Äá»c file tÄ©nh
lines_rdd = sc.textFile("input.txt")

# Word count trÃªn toÃ n bá»™ file
words_rdd = lines_rdd.flatMap(lambda line: line.split(" "))
word_counts_rdd = words_rdd.map(lambda word: (word, 1)) \
                           .reduceByKey(lambda x, y: x + y)

# Láº¥y káº¿t quáº£ má»™t láº§n
results = word_counts_rdd.collect()
print(results)  # [(word1, count1), (word2, count2), ...]
```

### Xá»­ lÃ½ vá»›i DStream (Streaming)
```python
# Äá»c stream liÃªn tá»¥c
lines_dstream = ssc.socketTextStream("localhost", 9999)

# Word count cho má»—i batch
words_dstream = lines_dstream.flatMap(lambda line: line.split(" "))
word_counts_dstream = words_dstream.map(lambda word: (word, 1)) \
                                   .reduceByKey(lambda x, y: x + y)

# In káº¿t quáº£ liÃªn tá»¥c
word_counts_dstream.pprint()

# Báº¯t Ä‘áº§u streaming
ssc.start()
ssc.awaitTermination()
```

## ğŸ­ Lifecycle (VÃ²ng Ä‘á»i)

### RDD Lifecycle
```
1. Create RDD â†’ 2. Transform â†’ 3. Action â†’ 4. Result
    (má»™t láº§n)        (lazy)       (eager)     (káº¿t thÃºc)
```

### DStream Lifecycle  
```
1. Create DStream â†’ 2. Transform â†’ 3. Output Op â†’ 4. Repeat...
    (setup)           (lazy)        (eager)       (liÃªn tá»¥c)
```

## ğŸ§ª Code Demo: RDD vs DStream

### RDD Example
```python
from pyspark import SparkContext

sc = SparkContext("local", "RDDExample")

# Táº¡o RDD tá»« dá»¯ liá»‡u cÃ³ sáºµn
data = ["hello world", "spark is awesome", "rdd processing"]
lines_rdd = sc.parallelize(data)

# Xá»­ lÃ½ word count
words_rdd = lines_rdd.flatMap(lambda line: line.split(" "))
word_counts_rdd = words_rdd.map(lambda word: (word, 1)) \
                           .reduceByKey(lambda x, y: x + y)

# Láº¥y káº¿t quáº£
print("RDD Results:")
for word, count in word_counts_rdd.collect():
    print(f"{word}: {count}")

sc.stop()
```

### DStream Example
```python
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

sc = SparkContext("local[2]", "DStreamExample")  
ssc = StreamingContext(sc, 2)

# Táº¡o DStream tá»« socket
lines_dstream = ssc.socketTextStream("localhost", 9999)

# Xá»­ lÃ½ word count liÃªn tá»¥c
words_dstream = lines_dstream.flatMap(lambda line: line.split(" "))
word_counts_dstream = words_dstream.map(lambda word: (word, 1)) \
                                   .reduceByKey(lambda x, y: x + y)

# In káº¿t quáº£ liÃªn tá»¥c
word_counts_dstream.pprint()

print("DStream Results (continuous):")
ssc.start()
ssc.awaitTermination()
```

## ğŸ¯ Khi nÃ o dÃ¹ng gÃ¬?

### Sá»­ dá»¥ng RDD khi:
- âœ… Xá»­ lÃ½ dá»¯ liá»‡u batch/historical
- âœ… Dá»¯ liá»‡u Ä‘Ã£ cÃ³ sáºµn trong files/databases  
- âœ… PhÃ¢n tÃ­ch one-time, ad-hoc queries
- âœ… Machine learning trÃªn static dataset

### Sá»­ dá»¥ng DStream khi:
- âœ… Xá»­ lÃ½ dá»¯ liá»‡u real-time/streaming
- âœ… Dá»¯ liá»‡u Ä‘áº¿n liÃªn tá»¥c (logs, sensors, events)
- âœ… Cáº§n káº¿t quáº£ ngay láº­p tá»©c
- âœ… Monitoring, alerting systems

## ğŸ”— Má»‘i quan há»‡

```python
# DStream BÃŠN TRONG chá»©a cÃ¡c RDD
dstream = ssc.socketTextStream("localhost", 9999)

# Má»—i batch interval, DStream táº¡o ra má»™t RDD má»›i
def process_rdd(time, rdd):
    print(f"Processing RDD at time {time}")
    print(f"RDD has {rdd.count()} elements")
    
    # CÃ³ thá»ƒ xá»­ lÃ½ RDD nhÆ° bÃ¬nh thÆ°á»ng
    results = rdd.collect()
    print(f"Results: {results}")

# Truy cáº­p RDD bÃªn trong DStream
dstream.foreachRDD(process_rdd)
```

## ğŸ’¡ Äiá»ƒm máº¥u chá»‘t

1. **DStream = Chuá»—i cÃ¡c RDD theo thá»i gian**
2. **RDD = Snapshot dá»¯ liá»‡u táº¡i má»™t thá»i Ä‘iá»ƒm**  
3. **DStream dÃ¹ng cho real-time, RDD dÃ¹ng cho batch**
4. **Cáº£ hai Ä‘á»u immutable vÃ  fault-tolerant**
5. **DStream operations Ã¡p dá»¥ng lÃªn má»—i RDD bÃªn trong**

---

ğŸ“ **Káº¿t luáº­n**: DStream lÃ  abstraction cao hÆ¡n cho streaming, nhÆ°ng bÃªn trong váº«n sá»­ dá»¥ng RDD Ä‘á»ƒ xá»­ lÃ½ tá»«ng micro-batch.