# Kiáº¿n trÃºc Spark Streaming

## ğŸ—ï¸ Tá»•ng quan kiáº¿n trÃºc

Spark Streaming sá»­ dá»¥ng mÃ´ hÃ¬nh **Micro-batching** Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u thá»i gian thá»±c:

```
Input Data Stream â†’ Micro-batches â†’ Spark Engine â†’ Output
```

## ğŸ“Š MÃ´ hÃ¬nh Micro-batching

### 1. **Input Layer - Lá»›p nháº­p dá»¯ liá»‡u**
```
Continuous Data Stream
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Batch 1 â”‚ Batch 2 â”‚ Batch 3 â”‚ Batch 4 â”‚ ...
â”‚  (2s)   â”‚  (2s)   â”‚  (2s)   â”‚  (2s)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. **Processing Layer - Lá»›p xá»­ lÃ½**
Má»—i micro-batch Ä‘Æ°á»£c xá»­ lÃ½ nhÆ° má»™t **RDD** thÃ´ng thÆ°á»ng:

```python
# Má»—i 2 giÃ¢y, Spark táº¡o má»™t RDD tá»« data stream
lines = ssc.socketTextStream("localhost", 9999)  # DStream
# lines táº¡i thá»i Ä‘iá»ƒm T1 â†’ RDD[String]
# lines táº¡i thá»i Ä‘iá»ƒm T2 â†’ RDD[String]  
# lines táº¡i thá»i Ä‘iá»ƒm T3 â†’ RDD[String]
```

### 3. **Output Layer - Lá»›p xuáº¥t dá»¯ liá»‡u**
Káº¿t quáº£ Ä‘Æ°á»£c ghi ra external systems hoáº·c console.

## ğŸ”„ Luá»“ng xá»­ lÃ½ chi tiáº¿t

### VÃ­ dá»¥ vá»›i Word Count:

```python
# 1. Táº¡o DStream tá»« socket
lines = ssc.socketTextStream("localhost", 9999)

# 2. Ãp dá»¥ng transformations (lazy evaluation)
words = lines.flatMap(lambda line: line.split(" "))
pairs = words.map(lambda word: (word, 1))
counts = pairs.reduceByKey(lambda x, y: x + y)

# 3. Output operation (trigger execution)
counts.pprint()
```

### Thá»±c thi:
```
T=0s:  Input: "hello world"
       â†’ RDD: ["hello", "world"] 
       â†’ Output: (hello,1), (world,1)

T=2s:  Input: "hello spark"  
       â†’ RDD: ["hello", "spark"]
       â†’ Output: (hello,1), (spark,1)

T=4s:  Input: "world spark"
       â†’ RDD: ["world", "spark"] 
       â†’ Output: (world,1), (spark,1)
```

## âš¡ Æ¯u Ä‘iá»ƒm cá»§a Micro-batching

### 1. **Fault Tolerance**
- Má»—i batch cÃ³ thá»ƒ Ä‘Æ°á»£c recompute náº¿u fail
- Lineage tracking nhÆ° RDD thÃ´ng thÆ°á»ng
- Exactly-once semantics

### 2. **Scalability** 
- Sá»­ dá»¥ng toÃ n bá»™ cluster cá»§a Spark
- Load balancing tá»± Ä‘á»™ng
- Dynamic resource allocation

### 3. **Integration**
- DÃ¹ng chung API vá»›i Spark Core vÃ  SQL
- CÃ³ thá»ƒ káº¿t há»£p vá»›i MLlib, GraphX
- Consistent programming model

## ğŸ¯ So sÃ¡nh vá»›i Stream Processing thuáº§n tÃºy

| Äáº·c Ä‘iá»ƒm | Micro-batching (Spark) | Pure Streaming (Storm, Flink) |
|----------|------------------------|--------------------------------|
| **Latency** | Cao hÆ¡n (seconds) | Tháº¥p hÆ¡n (milliseconds) |
| **Throughput** | Ráº¥t cao | Trung bÃ¬nh |
| **Fault Tolerance** | Excellent | Good |
| **Exactly-once** | Dá»… dÃ ng | Phá»©c táº¡p |
| **Learning Curve** | Dá»… (nhÆ° batch) | KhÃ³ hÆ¡n |

## ğŸ”§ CÃ¡c thÃ nh pháº§n chÃ­nh

### 1. **StreamingContext**
```python
ssc = StreamingContext(sc, batch_duration)
# batch_duration: thá»i gian cá»§a má»—i micro-batch
```

### 2. **DStream (Discretized Stream)**
- Abstraction cá»§a continuous data stream
- Internally: sequence of RDDs
- Immutable vÃ  distributed

### 3. **Transformations**
```python
# Stateless transformations
map(), filter(), flatMap(), union()

# Stateful transformations  
reduceByKey(), groupByKey(), join()

# Windowing transformations
window(), reduceByWindow()
```

### 4. **Output Operations**
```python
print()        # Console output
saveAsTextFiles()  # File output
foreachRDD()   # Custom output
```

## ğŸ“ˆ Performance Tuning

### 1. **Batch Duration**
```python
# QuÃ¡ ngáº¯n: overhead cao
ssc = StreamingContext(sc, 0.5)  # 500ms

# QuÃ¡ dÃ i: latency cao  
ssc = StreamingContext(sc, 10)   # 10s

# Tá»‘i Æ°u: cÃ¢n báº±ng throughput vs latency
ssc = StreamingContext(sc, 2)    # 2s (thÆ°á»ng dÃ¹ng)
```

### 2. **Parallelism**
```python
# TÄƒng parallelism cho input
lines = ssc.socketTextStream("localhost", 9999)
lines = lines.repartition(4)  # Chia thÃ nh 4 partitions

# Hoáº·c union multiple streams
stream1 = ssc.socketTextStream("host1", 9999) 
stream2 = ssc.socketTextStream("host2", 9999)
combined = stream1.union(stream2)
```

### 3. **Memory Management**
```python
# Thiáº¿t láº­p storage level
lines.persist(StorageLevel.MEMORY_AND_DISK_SER)

# Checkpoint cho stateful operations
ssc.checkpoint("hdfs://checkpoint")
```

## ğŸ” Monitoring & Debugging

### 1. **Spark UI**
- Streaming tab: batch processing times
- Jobs tab: detailed execution plan
- Storage tab: cached RDDs

### 2. **Logs**
```python
sc.setLogLevel("WARN")  # Giáº£m log noise
```

### 3. **Metrics**
```python
# Custom metrics
def count_records(time, rdd):
    count = rdd.count()
    print(f"Processed {count} records at {time}")

dstream.foreachRDD(count_records)
```

---

ğŸ“š **Tham kháº£o thÃªm**: [Spark Streaming Programming Guide](https://spark.apache.org/docs/latest/streaming-programming-guide.html)